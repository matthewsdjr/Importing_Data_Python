{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Locally Data \n",
    "Importar datos de manera local como: \n",
    "\n",
    "    1.  Archivo CSV o Excel o Matlab (.csv, .xls, .mat)\n",
    "    2. Base de Datos (SQLite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar Datos con el módulo pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leer el archivo CSV\n",
    "df = pd.read_csv('Financial Sample.csv')\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar Datos con el módulo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Abrir el archivo CSV\n",
    "with open('Financial Sample.csv', newline='') as csvfile:\n",
    "    # Leer el archivo CSV\n",
    "    reader = csv.reader(csvfile)\n",
    "    # Iterar sobre las filas del CSV\n",
    "    for row in reader:\n",
    "        print(', '.join(row))  # Imprimir cada fila\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar datos de la red\n",
    "Es bueno aprender a importar datos de manera local, pero no siempre será suficiente porque no obtendré los datos que necesito. Para esto tendré que importarlo desde la Web. \n",
    "\n",
    "Aquí aprenderemos a: \n",
    "- Import and locally save datasets from the web\n",
    "- Load datasets into Panda DataFrames\n",
    "- Make HTTP request (GET Request)\n",
    "- Scrape web data such as HTML \n",
    "- Parse HTML into usefull data (BeautifilSoup)\n",
    "- Use the urllib and request packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"urllib\" package\n",
    "- Provides interface for fetching data across the web \n",
    "- *urlopen()* accepts URLs instead of file names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
    "urlretrieve(url, 'winequality-white.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('winequality-white.csv', sep=';')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url, 'winequality-red.csv')\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening and reading flat files from the web \n",
    "You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using *pandas*. In particular, you can use the function *pd.read_csv()* with the URL as the first argument and the separator *sep* as the second argument.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodo *iloc*\n",
    "El método iloc en pandas se utiliza para la indexación basada en la posición entera de las filas y columnas de un DataFrame.\n",
    "\n",
    "En la expresión df.iloc[:, 0].hist(), se está aplicando iloc para seleccionar todas las filas de la primera columna del DataFrame df, y luego se está llamando al método hist() para trazar un histograma de los valores en esa columna.\n",
    "\n",
    "Aquí está desglosado cómo funciona:\n",
    "\n",
    "- *df.iloc[:, 0]*: Esto selecciona todas las filas (:) de la primera columna (0) del DataFrame df. El primer argumento (:) selecciona todas las filas, y el segundo argumento (0) selecciona la primera columna.\n",
    "- *.hist()*: Una vez que se ha seleccionado esa columna, se llama al método hist() para trazar un histograma de los valores en esa columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Read file into a DataFrame: df\n",
    "df = pd.read_csv(url, sep = ';')\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Plot first column of df\n",
    "df.iloc[:, 0].hist()\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing non-flat files from the web\n",
    "Congrats! You've just loaded a flat file from the web into a DataFrame without first saving it locally using the *pandas* function *pd.read_csv()*. This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you'll use *pd.read_excel()* to import an Excel spreadsheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En este caso estamos leyendo un archivo xls(excel) que contiene 2 hojas, para poder cargar todas las hojas especificamos 'None' en el 'sheet_name'\n",
    "- con el comando *keys()* imprimiremos los nombres de las hojas *['1700', '1900']*\n",
    "- Al final imprimimos los 5 primeros datos de la hoja llamada '1700'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "\n",
    "# Read in all sheets of Excel file: xls\n",
    "xls = pd.read_excel(url, sheet_name=None)\n",
    "\n",
    "# Print the sheetnames to the shell\n",
    "print(xls.keys())\n",
    "\n",
    "# Print the head of the first sheet (using its name, NOT its index)\n",
    "print(xls['1700'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing HTTP request in Python using urllib\n",
    "Now that you know the basics behind HTTP GET requests, it's time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, \"https://campus.datacamp.com/courses/1606/4135?ex=2\".\n",
    "\n",
    "In the next exercise, you'll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have just packaged and sent a GET request to \"https://campus.datacamp.com/courses/1606/4135?ex=2\" and then caught the response. You saw that such a response is a http.client.HTTPResponse object. The question remains: what can you do with this response?\n",
    "\n",
    "Well, as it came from an HTML page, you could read it to extract the HTML and, in fact, such a http.client.HTTPResponse object has an associated read() method. In this exercise, you'll build on your previous great work to extract the response and print the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extract the response: html\n",
    "html = response.read()\n",
    "\n",
    "# Print the html\n",
    "print(html)\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've got your head and hands around making HTTP requests using the urllib package, you're going to figure out how to do the same using the higher-level requests library. You'll once again be pinging DataCamp servers for their \"http://www.datacamp.com/teach/documentation\" page.\n",
    "\n",
    "Note that unlike in the previous exercises using urllib, you don't have to close the connection when using requests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Specify the url: url\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# Packages the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response: text\n",
    "text = r.text\n",
    "\n",
    "# Print the html\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup ! \n",
    "Beautiful Soup es una biblioteca de Python para extraer datos de documentos HTML y XML (incluyendo los que tienen un marcado incorrecto). Esta biblioteca crea un árbol con todos los elementos del documento y puede ser utilizado para extraer información. Por lo tanto, esta biblioteca es útil para realizar web scraping — extraer información de sitios web.2​\n",
    "\n",
    "Beautiful Soup no es un analizador de documentos (parser), sino que crea las estructuras de datos necesarias para manejar de manera sencilla los datos extraídos por los analizadores, los cuales no forman parte del paquete, sobre los que trabaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing HTML with BeautifulSoup\n",
    "In this interactive exercise, you'll learn how to use the BeautifulSoup package to parse, prettify and extract information from HTML. You'll scrape the data from the webpage of Guido van Rossum, Python's very own Benevolent Dictator for Life. In the following exercises, you'll prettify the HTML and then extract the text and the hyperlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the response\n",
    "print(pretty_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Turning a webpage into data using BeautifulSoup: getting the text\n",
    "As promised, in the following exercises, you'll learn the basics of extracting information from HTML soup. In this exercise, you'll figure out how to extract the text from the BDFL's webpage, along with printing the webpage's title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "guido_title = soup.title\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(guido_title)\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.get_text()\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(guido_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning a webpage into data using BeautifulSoup: getting the hyperlinks\n",
    "In this exercise, you'll figure out how to extract the URLs of the hyperlinks from the BDFL's webpage. In the process, you'll become close friends with the soup method find_all()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the method *find_all()* to find all hyperlinks in soup, remembering that hyperlinks are defined by the HTML tag 'a' but passed to *find_all()* without angle brackets; store the result in the variable a_tags.\n",
    "The variable *a_tags* is a results set: your job now is to enumerate over it, using a for loop and to print the actual *URLs* of the hyperlinks; to do this, for every element link in *a_tags*, you want to *print() link.get('href')*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Print the title of Guido's webpage\n",
    "print(soup.title)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags = soup.find_all('a')\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON (acrónimo de JavaScript Object Notation, 'notación de objeto de JavaScript') es un formato de texto sencillo para el intercambio de datos. Se trata de un subconjunto de la notación literal de objetos de JavaScript, aunque, debido a su amplia adopción como alternativa a XML, se considera un formato independiente del lenguaje.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and exploring a JSON\n",
    "Now that you know what a JSON is, you'll load one into your Python environment and explore it yourself. Here, you'll load the JSON 'a_movie.json' into the variable json_data, which will be a dictionary. You'll then explore the JSON contents by printing the key-value pairs of json_data to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON: json_data\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API's \n",
    "API es un conjunto de protocolos y rutinas para crear aplicaciones de software e interactuar con ellas. Permite que 2 programas de software puedan comunicarse entre si. Por ejm: Si quieres transmitir datos de Twitter en python usarias la API de Twitter. Si deseas automatizar la extraccion y procesamiento de la informacion de Wikipedia, se puede hacer la API de Wikipedia. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://www.omdbapi.com/?t=hackers'\n",
    "r = requests.get(url)\n",
    "json_data = r.json()\n",
    "print(json_data)\n",
    "for key, value in json_data.items():\n",
    "    print(key + ':', value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API requests\n",
    "Now it's your turn to pull some movie data down from the Open Movie Database (OMDB) using their API. The movie you'll query the API about is The Social Network. Recall that, in the video, to query the API about the movie Hackers, Hugo's query string was 'http://www.omdbapi.com/?t=hackers' and had a single argument t=hackers.\n",
    "\n",
    "Note: recently, OMDB has changed their API: you now also have to specify an API key. This means you'll have to add another argument to the URL: apikey=72bc447a.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Import the requests package.\n",
    "- Assign to the variable url the URL of interest in order to query 'http://www.omdbapi.com' for the data corresponding to the movie The Social Network. The query string should have two arguments: apikey=72bc447a and t=the+social+network. You can combine them as follows: apikey=72bc447a&t=the+social+network.\n",
    "- Print the text of the response object r by using its text attribute and passing the result to the print() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Title\":\"The Social Network\",\"Year\":\"2010\",\"Rated\":\"PG-13\",\"Released\":\"01 Oct 2010\",\"Runtime\":\"120 min\",\"Genre\":\"Biography, Drama\",\"Director\":\"David Fincher\",\"Writer\":\"Aaron Sorkin, Ben Mezrich\",\"Actors\":\"Jesse Eisenberg, Andrew Garfield, Justin Timberlake\",\"Plot\":\"As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea and by the co-founder who was later squeezed out of the business.\",\"Language\":\"English, French\",\"Country\":\"United States\",\"Awards\":\"Won 3 Oscars. 173 wins & 187 nominations total\",\"Poster\":\"https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\",\"Ratings\":[{\"Source\":\"Internet Movie Database\",\"Value\":\"7.8/10\"},{\"Source\":\"Rotten Tomatoes\",\"Value\":\"96%\"},{\"Source\":\"Metacritic\",\"Value\":\"95/100\"}],\"Metascore\":\"95\",\"imdbRating\":\"7.8\",\"imdbVotes\":\"754,796\",\"imdbID\":\"tt1285016\",\"Type\":\"movie\",\"DVD\":\"05 Jun 2012\",\"BoxOffice\":\"$96,962,694\",\"Production\":\"N/A\",\"Website\":\"N/A\",\"Response\":\"True\"}\n",
      "Title: The Social Network\n",
      "Year: 2010\n",
      "Rated: PG-13\n",
      "Released: 01 Oct 2010\n",
      "Runtime: 120 min\n",
      "Genre: Biography, Drama\n",
      "Director: David Fincher\n",
      "Writer: Aaron Sorkin, Ben Mezrich\n",
      "Actors: Jesse Eisenberg, Andrew Garfield, Justin Timberlake\n",
      "Plot: As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea and by the co-founder who was later squeezed out of the business.\n",
      "Language: English, French\n",
      "Country: United States\n",
      "Awards: Won 3 Oscars. 173 wins & 187 nominations total\n",
      "Poster: https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\n",
      "Ratings: [{'Source': 'Internet Movie Database', 'Value': '7.8/10'}, {'Source': 'Rotten Tomatoes', 'Value': '96%'}, {'Source': 'Metacritic', 'Value': '95/100'}]\n",
      "Metascore: 95\n",
      "imdbRating: 7.8\n",
      "imdbVotes: 754,796\n",
      "imdbID: tt1285016\n",
      "Type: movie\n",
      "DVD: 05 Jun 2012\n",
      "BoxOffice: $96,962,694\n",
      "Production: N/A\n",
      "Website: N/A\n",
      "Response: True\n"
     ]
    }
   ],
   "source": [
    "# Import requests package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Print the text of the response\n",
    "print(r.text)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "\n",
    "for k, value in json_data.items():\n",
    "    print(k + ':', value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking out the Wikipedia API\n",
    "You're doing so well and having so much fun that we're going to throw one more API at you: the Wikipedia API (documented here). You'll figure out how to find and extract information from the Wikipedia page for Pizza. What gets a bit wild here is that your query will return nested JSONs, that is, JSONs with JSONs, but Python can handle that because it will translate them into dictionaries within dictionaries.\n",
    "\n",
    "The URL that requests the relevant query from the Wikipedia API is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: \n",
    "- Assign the relevant URL to the variable url.\n",
    "- Apply the json() method to the response object r and store the resulting dictionary in the variable json_data.\n",
    "- The variable pizza_extract holds the HTML of an extract from Wikipedia's Pizza page as a string; use the function print() to print this string to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "\n",
    "# Print the Wikipedia page extract\n",
    "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
    "print(pizza_extract)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Twitter API and Authentication\n",
    "As a final deep dive, you're going to stream data from the Twitter API. You'll learn how to filter incoming tweets for keywords, you'll learn about the principles of API authentication and OAuth. You'll also learn the basics of the package tweepy, which many people in PythonLand use to interact with the Twitter API.\n",
    "\n",
    "### Streaming tweets\n",
    "It's time to stream some tweets! Your task is to create the Streamobject and to filter tweets according to particular keywords. tweepy has been imported for you.\n",
    "\n",
    "Instructions: \n",
    "- Create your Stream object with the credentials given.\n",
    "- Filter your Stream variable for the keywords \"clinton\", \"trump\", \"sanders\", and \"cruz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy, json\n",
    "\n",
    "# Store credentials in relevant variables\n",
    "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
    "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
    "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
    "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
    "\n",
    "# Create your Stream object with credentials\n",
    "stream = tweepy.Stream(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "# Filter your Stream variable\n",
    "stream.filter(track = ['clinton', 'trump', 'sanders', 'cruz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Failed On, 403 Forbidden\n",
      "453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "\n",
    "consumer_key = \"KVRVfDFf0oFPtK94pPMd0ZjEu\" #Your API/Consumer key \n",
    "consumer_secret = \"ZkTM47w3pQjDMN98qNwHubayxRP8x9mx3Je1ucVAsfPnnpN8I9\" #Your API/Consumer Secret Key\n",
    "access_token = \"1056483163-YIq6v2LCsQLwE0dZz9bOlhdaccnXlIV8HUNpk9Y\"    #Your Access token key\n",
    "access_token_secret = \"89ncfIq3657XzGJ7Vgod34cRCIs6fBXV2FBFhcgrT6SV4\" #Your Access token Secret key\n",
    "\n",
    "#Pass in our twitter API authentication key\n",
    "auth = tweepy.OAuth1UserHandler(\n",
    "    consumer_key, consumer_secret,\n",
    "    access_token, access_token_secret\n",
    ")\n",
    "#api = tweepy.API(auth, wait_on_rate_limit=True, api_version='1.1')\n",
    "\n",
    "\n",
    "#Instantiate the tweepy API\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "\n",
    "search_query = \"'Elon Musk''fired'-filter:retweets AND -filter:replies AND -filter:links\"\n",
    "no_of_tweets = 100\n",
    "\n",
    "try:\n",
    "    #The number of tweets we want to retrieved from the search\n",
    "    tweets = api.search_tweets(q=search_query, lang=\"en\", count=no_of_tweets, tweet_mode ='extended')\n",
    "    \n",
    "    #Pulling Some attributes from the tweet\n",
    "    attributes_container = [[tweet.user.name, tweet.created_at, tweet.favorite_count, tweet.source, tweet.full_text] for tweet in tweets]\n",
    "\n",
    "    #Creation of column list to rename the columns in the dataframe\n",
    "    columns = [\"User\", \"Date Created\", \"Number of Likes\", \"Source of Tweet\", \"Tweet\"]\n",
    "    \n",
    "    #Creation of Dataframe\n",
    "    tweets_df = pd.DataFrame(attributes_container, columns=columns)\n",
    "except BaseException as e:\n",
    "    print('Status Failed On,',str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tweepy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\n",
      "------------------ -----------\n",
      "asttokens          2.4.1\n",
      "beautifulsoup4     4.12.3\n",
      "certifi            2024.2.2\n",
      "charset-normalizer 3.3.2\n",
      "colorama           0.4.6\n",
      "comm               0.2.2\n",
      "contourpy          1.2.1\n",
      "cycler             0.12.1\n",
      "debugpy            1.8.1\n",
      "decorator          5.1.1\n",
      "executing          2.0.1\n",
      "fonttools          4.51.0\n",
      "idna               3.7\n",
      "ipykernel          6.29.4\n",
      "ipython            8.23.0\n",
      "jedi               0.19.1\n",
      "jupyter_client     8.6.1\n",
      "jupyter_core       5.7.2\n",
      "kiwisolver         1.4.5\n",
      "matplotlib         3.8.4\n",
      "matplotlib-inline  0.1.6\n",
      "nest-asyncio       1.6.0\n",
      "numpy              1.26.4\n",
      "oauthlib           3.2.2\n",
      "opencv-python      4.9.0.80\n",
      "packaging          24.0\n",
      "pandas             2.2.2\n",
      "parso              0.8.4\n",
      "pillow             10.3.0\n",
      "pip                24.0\n",
      "platformdirs       4.2.0\n",
      "prompt-toolkit     3.0.43\n",
      "psutil             5.9.8\n",
      "pure-eval          0.2.2\n",
      "Pygments           2.17.2\n",
      "pyparsing          3.1.2\n",
      "python-dateutil    2.9.0.post0\n",
      "pytz               2024.1\n",
      "pywin32            306\n",
      "pyzmq              25.1.2\n",
      "requests           2.31.0\n",
      "requests-oauthlib  1.3.1\n",
      "six                1.16.0\n",
      "soupsieve          2.5\n",
      "stack-data         0.6.3\n",
      "tornado            6.4\n",
      "traitlets          5.14.2\n",
      "tweepy             4.10.0\n",
      "tzdata             2024.1\n",
      "urllib3            2.2.1\n",
      "wcwidth            0.2.13\n",
      "xlrd               2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the URLs endpoints\n",
    "joke_url = 'https://icanhazdadjoke.com'\n",
    "iss_url = 'https://api.wheretheiss.at/v1/satellites/25544'\n",
    "\n",
    "# This is setting up a header - metadata for your api request\n",
    "my_header = {'Accept':'application/json'}\n",
    "\n",
    "#API call\n",
    "results = requests.get(joke_url, headers=my_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_result = results.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'bprz5wXSSvc', 'joke': 'Why did the scarecrow win an award? Because he was outstanding in his field.', 'status': 200}\n"
     ]
    }
   ],
   "source": [
    "print(json_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the scarecrow win an award? Because he was outstanding in his field.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_result['joke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'iss',\n",
       " 'id': 25544,\n",
       " 'latitude': 4.7297815024866,\n",
       " 'longitude': 121.52644698227,\n",
       " 'altitude': 414.86399404544,\n",
       " 'velocity': 27592.701935199,\n",
       " 'visibility': 'daylight',\n",
       " 'footprint': 4481.2409190064,\n",
       " 'timestamp': 1713126751,\n",
       " 'daynum': 2460415.3559144,\n",
       " 'solar_lat': 9.8169152542643,\n",
       " 'solar_lon': 231.89044275785,\n",
       " 'units': 'kilometers'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the ISS URL CALL\n",
    "iss_results = requests.get(iss_url, headers=my_header)\n",
    "iss_results.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
